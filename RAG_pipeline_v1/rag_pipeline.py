from openai import OpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

from rag_config import RAGConfig
from rag_retriever import RAGRetriever


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ - ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""

    def __init__(self, config: RAGConfig = None, model: str = None, top_k: int = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: RAG ì„¤ì • ê°ì²´
            model: LLM ëª¨ë¸ëª… (Noneì´ë©´ config ê¸°ë³¸ê°’)
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ config ê¸°ë³¸ê°’)
        """
        self.config = config or RAGConfig()
        self.model = model or self.config.LLM_MODEL_NAME
        self.top_k = top_k or self.config.DEFAULT_TOP_K

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = OpenAI(api_key=self.config.OPENAI_API_KEY)

        # Retriever ì´ˆê¸°í™”
        self.retriever = RAGRetriever(config=self.config)

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.chat_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ê³µê³µê¸°ê´€ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶• ì‚¬ì—… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ê·œì¹™:
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
3. ë‹µë³€ ì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""),

            ("user", """ê²€ìƒ‰ëœ ë¬¸ì„œ:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}""")
        ])

        print(f"RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")

    def _format_context(self, retrieved_docs: list) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        context_parts = []

        for i, doc in enumerate(retrieved_docs, 1):
            context_parts.append(
                f"[ë¬¸ì„œ {i}]\n"
                f"ì¶œì²˜: {doc['filename']}\n"
                f"ë°œì£¼ê¸°ê´€: {doc['organization']}\n"
                f"ê´€ë ¨ë„: {doc['relevance_score']:.3f}\n"
                f"ë‚´ìš©: {doc['content']}\n"
            )

        return "\n".join(context_parts)

    def _build_prompt(self, query: str, retrieved_docs: list):
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° OpenAI API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # context ìƒì„±
        context = self._format_context(retrieved_docs)

        # ë©”ì‹œì§€ ìƒì„±
        messages = self.chat_prompt.format_messages(
            context=context,
            query=query
        )

        # OpenAI API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        api_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                api_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                api_messages.append({"role": "user", "content": msg.content})

        return api_messages

    def generate_answer(
        self,
        query: str,
        temperature: float = None,
        max_tokens: int = None
    ):
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            temperature: LLM temperature (Noneì´ë©´ config ê¸°ë³¸ê°’)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ config ê¸°ë³¸ê°’)
            
        Returns:
            ë‹µë³€ ë° ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        if temperature is None:
            temperature = self.config.DEFAULT_TEMPERATURE
        if max_tokens is None:
            max_tokens = self.config.DEFAULT_MAX_TOKENS

        # 1. ê²€ìƒ‰
        retrieved_docs = self.retriever.search(query, top_k=self.top_k)

        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        api_messages = self._build_prompt(query, retrieved_docs)

        # 3. LLM í˜¸ì¶œ
        response = self.client.chat.completions.create(
            model=self.model,
            messages=api_messages,
            temperature=temperature,
            max_tokens=max_tokens
        )

        # ë‹µë³€ ì¶”ì¶œ
        answer = response.choices[0].message.content

        # 4. ê²°ê³¼ êµ¬ì¡°í™”
        result = {
            'query': query,
            'answer': answer,
            'sources': [
                {
                    'filename': doc['filename'],
                    'organization': doc['organization'],
                    'relevance_score': doc['relevance_score'],
                    'content_preview': doc['content'][:100] + "..."
                }
                for doc in retrieved_docs
            ],
            'model': self.model,
            'usage': {
                'prompt_tokens': response.usage.prompt_tokens,
                'completion_tokens': response.usage.completion_tokens,
                'total_tokens': response.usage.total_tokens
            }
        }

        return result

    def print_result(self, result: dict):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"ì§ˆë¬¸: {result['query']}")
        print("="*60)

        print(f"\nğŸ’¬ ë‹µë³€:\n{result['answer']}")

        print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ ({len(result['sources'])}ê°œ):")
        for i, source in enumerate(result['sources'], 1):
            print(f"  [{i}] {source['filename']}")
            print(f"      ë°œì£¼ê¸°ê´€: {source['organization']}")
            print(f"      ê´€ë ¨ë„: {source['relevance_score']:.3f}")

        print(f"\nğŸ“Š ì‚¬ìš©ëŸ‰:")
        print(f"  ëª¨ë¸: {result['model']}")
        print(f"  í† í°: {result['usage']['total_tokens']} "
              f"(ì…ë ¥: {result['usage']['prompt_tokens']}, "
              f"ì¶œë ¥: {result['usage']['completion_tokens']})")

        print("="*60)